# RFC-001: Persistent Data Structure Architecture for ZatDB

- **Status:** Proposed
- **Authors:** ZatDB Core Team
- **Date:** 2026-02-22
- **Supersedes:** Implementation Design v0.2, Sections 3–6, 8

---

## Summary

This RFC proposes replacing ZatDB’s current LMDB-style mutable COW B+ tree with
dual meta pages with a **fully persistent (FP-sense) B+ tree** architecture where
every transaction produces a new set of roots via structural sharing, no pages are
ever freed or overwritten, and the file is an append-only history of every database
state that has ever existed.

This eliminates B+ tree delete operations, free page tracking, the dual meta page
scheme, transaction IDs in index keys, and retraction-resolution logic in the query
path. Time travel becomes pointer indirection rather than filtering. The file
format changes from “two alternating meta pages” to “append-only root log.”

---

## Motivation

### Problems with the current implementation (Steps 0–9)

The current implementation, built through Step 9 of the original spec, uses an
LMDB-inspired COW B+ tree with dual meta pages. It has the following issues:

**1. B+ tree delete is implemented but shouldn’t exist.**

The EAV/AVE/VAE indexes store current-state-only, meaning cardinality-one
replacement and explicit retractions must remove the old entry from the tree
and insert a new one. This requires COW delete (path-copy with entry removal),
merge-on-underflow, and free page tracking. These are the most complex and
bug-prone parts of the B+ tree engine.

For a database whose core philosophy is “immutable facts,” having mutable
indexes is a contradiction.

**2. Free page tracking adds complexity for a feature we don’t need.**

Step 6 implemented a FreeDB B+ tree to track pages freed by COW operations.
This is self-referential (the FreeDB needs pages to record freed pages) and
requires tracking the oldest active reader to know when pages are safe to
reclaim. This machinery exists solely because we delete from indexes.

**3. Dual meta pages only remember two versions.**

The current commit protocol alternates between meta page 0 and meta page 1.
This gives crash safety (if one is corrupt, fall back to the other) but
destroys all history beyond the previous transaction. A database that stores
immutable facts should not destroy its own history at the storage layer.

**4. Time travel requires the TxLog as a separate structure.**

Because EAV/AVE/VAE contain only current state, answering “what was entity 42
at tx 500?” requires consulting the TxLog index and replaying history. The
`asOf()` implementation is complex and slow for old time points.

**5. Index keys carry Tx and Op for the TxLog but not for EAV/AVE/VAE.**

This inconsistency means two different datom encodings, two different
comparison functions, and the retraction of a datom produces a _different_
kind of write depending on the index (delete in EAV, insert in TxLog).

### What the persistent data structure approach gives us

Every problem above disappears:

| Problem            | Current                                  | Proposed                                                     |
| ------------------ | ---------------------------------------- | ------------------------------------------------------------ |
| B+ tree delete     | Required for cardinality-one replacement | Eliminated — COW insert only, old value persists in old root |
| Free page tracking | FreeDB B+ tree, reader slot tracking     | Eliminated — pages never freed                               |
| History retention  | Two versions (dual meta)                 | All versions (root log)                                      |
| Time travel        | TxLog replay + filtering                 | Pick a different root — O(1)                                 |
| Datom encoding     | Two formats (with/without Tx)            | One format — no Tx/Op in tree keys                           |

---

## Design

### Core Principle

> **A database value is a set of B+ tree root page numbers. A transaction
> produces new roots via path-copying. Old roots remain valid because old
> pages are never overwritten. The file is an append-only history of roots.**

This is exactly how persistent data structures work in functional programming
(Clojure’s persistent vectors, Haskell’s Data.Map), except our “heap” is a
memory-mapped file and our “nodes” are disk pages.

### Architecture

```
┌──────────────────────────────────────────────────┐
│                   User API Layer                  │
│  db.transact()  db.query()  db.pull()  db.asOf() │
├──────────────────────────────────────────────────┤
│              Transaction Processor                │
│  tempid resolution → schema validation →         │
│  COW tree inserts → root log append              │
├──────────────────────────────────────────────────┤
│        Persistent Index Trees (3 indexes)         │
│  EAV (all datoms)   AVE (selective)  VAE (refs)   │
│  Each tree: current-state at its root             │
│  Old roots = old database states (shared pages)   │
├──────────────────────────────────────────────────┤
│           Append-Only Transaction Log             │
│  Sequential record of every assertion/retraction  │
│  For history queries and auditing only            │
├──────────────────────────────────────────────────┤
│         Persistent COW B+ Tree Engine             │
│  insert only → path copy → new root              │
│  NO delete, NO merge, NO free pages              │
├──────────────────────────────────────────────────┤
│              Page Manager / mmap                  │
│  file open → mmap → page read (zero-copy) →      │
│  page write (pwrite + append) → fsync            │
├──────────────────────────────────────────────────┤
│               Single .zat File                    │
│  [Header][Pages...grow forever...][Root Log]      │
└──────────────────────────────────────────────────┘
```

### 1. File Format Changes

#### 1.1 Remove dual meta pages

Replace meta page 0 / meta page 1 with a single immutable **file header** (page 0)
and an **append-only root log** at a known location.

**New file header (page 0) — written once at creation, never modified except
`root_log_tail`:**

```
Offset  Size  Field
0       4     magic: 0x5A415444 ("ZATD")
4       4     version: u32 (2) ← bumped from 1
8       4     page_size: u32
12      4     flags: u32
16      8     root_log_head: u64 (page number of first root log page)
24      8     root_log_tail: u64 (page number of latest root log page) ← ONLY mutable field
32      8     next_page: u64 (next free page = file size / page_size)
40      4     checksum: u32 (CRC-32C of bytes 0..39)
44      ...   padding to page_size
```

The `root_log_tail` pointer is the **only mutable field in the entire file.** It is
updated atomically (8-byte aligned write) on each commit. If torn, we scan backward
from the last known-good root log page.

#### 1.2 Root Log

An append-only chain of pages, each containing root records:

**Root log page layout:**

```
[PageHeader: 8 bytes]
[prev_page: 8 bytes]            // previous root log page (0 = first)
[entry_count: 2 bytes]
[entries...]
    [tx_id:          8 bytes]
    [timestamp:      8 bytes]   // epoch microseconds
    [eav_root:       8 bytes]   // page number of EAV B+ tree root
    [ave_root:       8 bytes]   // page number of AVE B+ tree root
    [vae_root:       8 bytes]   // page number of VAE B+ tree root
    [txlog_tail:     8 bytes]   // page number of latest TxLog page
    [entity_id_next: 8 bytes]   // next available entity IDs (per-partition)
    [datom_count:    8 bytes]   // cumulative datom count
    [checksum:       4 bytes]   // CRC-32C of this entry
                                // Total: 76 bytes per entry
```

At 4KB page size, each root log page holds ~52 entries. At 16KB, ~213 entries.

**Crash safety:** New pages are written via `pwrite()` + `fsync()` before the
root log entry is written. The root log entry itself is appended and fsynced.
If a crash occurs:

- Before root log append: new pages are orphaned but file is consistent at
  previous root. No corruption.
- During root log append: checksum on the entry detects partial write. The
  entry is ignored, previous entry is the latest valid state.

This is strictly safer than dual meta pages because we never overwrite anything.

#### 1.3 Remove page type 0x04 (Free)

The `Free` page type and `index_id=4` (Free) are removed. The FreeDB B+ tree
is removed. Pages are never freed.

#### 1.4 Transaction Log

The TxLog changes from a B+ tree to a simple **append-only page chain**:

```
[PageHeader: 8 bytes]
[prev_page: 8 bytes]
[entry_count: 2 bytes]
[entries...]
    [tx_id: 8 bytes][E: 8 bytes][A: 8 bytes]
    [value_tag: 1 byte][value_payload: variable]
    [op: 1 byte]  // true=assert, false=retract
```

This is not a B+ tree — just linked pages, newest first. Queries on the TxLog
(history, audit) scan backward from the tail. For random access by tx_id, the
root log provides direct pointers to the TxLog state at each transaction.

### 2. B+ Tree Changes

#### 2.1 Remove delete operation

The `BPlusTree.delete()` method is removed entirely, along with:

- Merge-on-underflow logic
- Underflow detection
- Sibling redistribution
- All code paths that reduce entry count in a page

The B+ tree supports exactly two operations:

- **lookup(key) → ?value** (read, via mmap, zero-copy)
- **insert(key, value) → new_root_page** (write, via COW path-copy)

And the iterator:

- **seek(key) → Iterator** (positioned at first entry ≥ key)
- **Iterator.next() → ?(key, value)**

#### 2.2 Replace operation for cardinality-one

For cardinality-one attributes, we need to replace a value at an existing (E, A)
position. This is a **COW replace**, not a delete-then-insert:

```zig
/// Replace the value for an existing key, or insert if not present.
/// Returns new root page number.
pub fn put(self: *BPlusTree, key: []const u8, value: []const u8) !u64 {
    var path = self.descend(key);
    var new_leaf = try self.fm.cowCopy(path.leaf());

    if (findInLeaf(new_leaf, key)) |idx| {
        // Key exists — replace value in-place within the COW'd page
        replaceEntryValue(new_leaf, idx, value);
    } else {
        // Key doesn't exist — insert
        insertEntry(new_leaf, key, value);
        if (pageOverflow(new_leaf)) {
            return self.splitAndPropagate(&path, new_leaf);
        }
    }
    return self.propagateNewChild(&path, new_leaf);
}
```

This is semantically identical to `insert` but overwrites an existing entry’s
value if the key matches. The old leaf page (with the old value) persists in
old roots via structural sharing.

**This is NOT a delete.** The page still has the same number of entries (or one
more). There is no underflow. The old page remains valid for old roots.

#### 2.3 Remove free page tracking from allocator

Page allocation simplifies to:

```zig
pub fn allocPage(self: *FileManager) !u64 {
    const pgno = self.next_page;
    self.next_page += 1;
    try self.extendFile(self.next_page * self.page_size);
    return pgno;
}
```

No FreeDB consultation. No reader-slot checking. Monotonically increasing.

### 3. Index Key Encoding Changes

#### 3.1 Remove Tx and Op from EAV/AVE/VAE keys

Since each root represents current state at a point in time, there is no need
for temporal information in the index keys. The indexes answer “what is true
at this root?” — not “what was true at tx N?”

**EAV Index (unchanged from current implementation):**

```
Key:    [E: 8 bytes BE]
Value:  [A: 8 bytes BE][value_tag: 1 byte][value_payload: variable]
```

No Tx. No Op. This is already what’s implemented — the current spec also
excludes Tx from EAV. **No encoding change needed.**

**AVE Index (unchanged):**

```
Key:    [A: 8 bytes BE]
Value:  [value_tag: 1 byte][value_payload: variable][E: 8 bytes BE]
```

**VAE Index (unchanged):**

```
Key:    [V(ref): 8 bytes BE]
Value:  [A: 8 bytes BE][E: 8 bytes BE]
```

#### 3.2 TxLog encoding adds Op field

The TxLog is the only structure that records assertions AND retractions:

```
[tx_id: 8][E: 8][A: 8][value_tag: 1][value_payload: var][op: 1]
```

### 4. Transaction Processing Changes

#### 4.1 Cardinality-one replacement

**Current implementation (Step 9):**

```
1. Look up old value in EAV: seek(entity, attr)
2. If exists and different from new value:
   a. Delete old (E, A, old_V) from EAV tree → COW delete path
   b. Delete old entry from AVE tree (if indexed) → COW delete path
   c. Delete old entry from VAE tree (if ref) → COW delete path
   d. Insert retraction datom into TxLog
3. Insert new (E, A, new_V) into EAV tree → COW insert
4. Insert into AVE, VAE as appropriate
5. Insert assertion datom into TxLog
```

**Proposed:**

```
1. Look up old value in EAV: seek(entity, attr)
2. If exists and different from new value:
   a. put(E, A, new_V) in EAV tree → COW replace (old page preserved)
   b. Remove old + insert new in AVE tree → COW replace
   c. Handle VAE: remove old ref + insert new ref → COW replace
   d. Append retraction AND assertion to TxLog
3. If not exists:
   a. insert(E, A, new_V) in EAV tree → COW insert
   b. Insert into AVE, VAE as appropriate
   c. Append assertion to TxLog
```

The critical difference: step 2a uses `put` (COW replace), not delete+insert.
The old leaf page, containing the old value, remains reachable from old roots.
**No B+ tree delete is ever called.**

#### 4.2 Explicit retractions

When the user explicitly retracts a fact `[:db/retract E A V]`:

**Current implementation:**

```
1. Verify (E, A, V) exists in EAV
2. Delete from EAV → COW delete
3. Delete from AVE/VAE → COW delete
4. Insert retraction into TxLog
```

**Proposed:**

```
1. Verify (E, A, V) exists in EAV
2. Remove entry from EAV via COW:
   - COW-copy the leaf
   - Remove the (A, V) duplicate entry from the DUPSORT group for E
   - If this was the last attribute, remove the E key entirely
   - Propagate new leaf upward via path-copy
3. Same for AVE/VAE
4. Append retraction to TxLog
```

Wait — this IS a delete from the leaf. For explicit retractions and
cardinality-many removal, we need to remove a specific (A, V) pair from
an entity’s DUPSORT group.

**Resolution:** We need a `remove` operation, but it’s a constrained one:
remove a specific duplicate value from a DUPSORT key group. This is
**not the same as B+ tree key delete** — the key (entity ID) may still
have other values. And even in the degenerate case where it’s the last
value, we’re just COW-copying a leaf with one fewer entry. No underflow
handling is needed because:

1. Old roots still reference the old leaf (with the entry present)
1. The new leaf may be underfull — that’s fine, it just wastes some space
1. No merging with siblings is ever needed (append-only file, no reclamation)

So we define a limited operation:

```zig
/// Remove a specific (key, dup_value) pair from a DUPSORT tree.
/// Returns new root. The old root still sees the old data.
/// No underflow handling — pages may become sparse.
pub fn removeDup(self: *BPlusTree, key: []const u8, dup_value: []const u8) !u64 {
    var path = self.descend(key);
    var new_leaf = try self.fm.cowCopy(path.leaf());

    removeDupEntry(new_leaf, key, dup_value);
    // No underflow check. No merge. Page may be nearly empty — that's OK.
    return self.propagateNewChild(&path, new_leaf);
}
```

This is drastically simpler than full B+ tree delete:

- No merge-on-underflow
- No sibling redistribution
- No recursive deletion of empty internal nodes
- No free page tracking

The worst case is pages with wasted space after many retractions. This is
addressed by optional compaction (future work).

#### 4.3 Commit protocol

**Current (dual meta page):**

```
1. pwrite() all dirty COW pages
2. fsync()
3. Overwrite the older meta page with new roots
4. fsync()
```

**Proposed (root log append):**

```
1. pwrite() all dirty COW pages
2. pwrite() TxLog append page(s)
3. fsync()
4. Append root record to root log (pwrite to root log tail page)
5. Update file header root_log_tail pointer
6. fsync()
```

If crash before step 4: orphaned pages, previous root still valid.
If crash during step 4: checksum on root record detects partial write.
If crash before step 6: root log page has the record but header doesn’t
point to it — on recovery, scan forward from last known tail to find it.

### 5. Time Travel

Time travel is now trivial:

```zig
pub fn asOf(self: *Database, tx_id: u64) DatabaseValue {
    // Binary search the root log for the entry with this tx_id
    const entry = self.root_log.findEntry(tx_id);
    return DatabaseValue{
        .eav = BPlusTree.init(entry.eav_root, self.fm),
        .ave = BPlusTree.init(entry.ave_root, self.fm),
        .vae = BPlusTree.init(entry.vae_root, self.fm),
    };
}
```

The returned `DatabaseValue` reads from old roots, which point to pages
that still exist in the file (nothing is ever overwritten). Structural sharing
means most pages are shared between the current database and the historical one.

**`since(tx_id)`** requires comparing two database values. This is more complex
but still straightforward: iterate both the old and current EAV trees and yield
differences. This is a merge-join of two sorted iterators.

**`history(entity, attr)`** uses the TxLog: scan for all entries matching (E, A)
to see every assertion and retraction over time.

### 6. Concurrency Model Changes

#### 6.1 Remove reader slot table

The reader slot table existed to prevent free page reclamation while readers
are active. Since pages are never freed, it is unnecessary.

Readers simply read from whatever root they obtained at the start of their
read transaction. Those pages will always exist. No coordination needed.

```zig
pub fn beginRead(self: *Database) DatabaseValue {
    // Just snapshot the current root — that's it
    const entry = self.root_log.latest();
    return DatabaseValue{
        .eav = BPlusTree.init(entry.eav_root, self.fm),
        .ave = BPlusTree.init(entry.ave_root, self.fm),
        .vae = BPlusTree.init(entry.vae_root, self.fm),
    };
}
// No endRead() needed — there's nothing to release
```

#### 6.2 Writer serialization unchanged

Single writer, serialized by mutex. This is unchanged from the current design.

---

## Migration Path

### What existing code is preserved

| Component                      | Status                          | Notes                                              |
| ------------------------------ | ------------------------------- | -------------------------------------------------- |
| Value encoding (Step 1)        | **Keep as-is**                  | No changes needed                                  |
| Page primitives (Step 2)       | **Keep, remove Free page type** | Remove page_type 0x04                              |
| File manager (Step 3)          | **Modify**                      | Remove dual meta, add root log, simplify allocator |
| B+ tree read path (Step 4)     | **Keep as-is**                  | lookup, seek, iterator unchanged                   |
| B+ tree write/COW (Step 5)     | **Simplify**                    | Remove delete(), add put(), add removeDup()        |
| Free page tracking (Step 6)    | **Remove entirely**             | Delete all FreeDB code                             |
| Schema & bootstrap (Step 7)    | **Keep as-is**                  | No changes needed                                  |
| Index manager (Step 8)         | **Modify**                      | Replace delete calls with put/removeDup            |
| Transaction processor (Step 9) | **Modify**                      | New commit protocol, cardinality-one via put       |

### Estimated migration effort

| Task                                                  | Days         |
| ----------------------------------------------------- | ------------ |
| Remove FreeDB, free page tracking, reader slots       | 1            |
| Replace dual meta pages with root log                 | 2            |
| Add `put()` (COW replace) to B+ tree                  | 1            |
| Add `removeDup()` to B+ tree                          | 1            |
| Remove `delete()` and merge-on-underflow from B+ tree | 1            |
| Update transaction processor commit protocol          | 1            |
| Update index manager to use put/removeDup             | 1            |
| Replace TxLog B+ tree with append-only page chain     | 1            |
| Implement root log read (for asOf)                    | 1            |
| Update and run full test suite                        | 2            |
| **Total**                                             | **~12 days** |

### File format version bump

The file format version changes from 1 to 2. Version 1 files are not compatible.
Since we haven’t shipped, no migration tool is needed — users recreate databases.

---

## Impact on Future Steps

### Steps 10–11 (Datalog parser + query executor): Simplified

Query execution no longer needs retraction-resolution filtering. A query against
a `DatabaseValue` (a set of tree roots) sees exactly the current state at that
root. Pattern clauses translate directly to B+ tree lookups.

### Step 12 (Time travel): Drastically simplified

`asOf` becomes root log lookup + tree initialization. No TxLog scanning.
`history` still needs TxLog, but is a simpler append-only scan.

### Step 13 (Concurrent readers): Drastically simplified

No reader slots, no coordination. Readers snapshot a root and go. The only
remaining concern is extending the mmap when the file grows, which requires
a brief lock or remapping.

### Steps 18–19 (Compression, Bloom filters): Unchanged

These apply to the B+ tree internals and are orthogonal to this RFC.

### New future work: Compaction / GC

The file grows forever. Eventually, users may want to reclaim space from
historical versions they no longer need. This is future work:

```zig
/// Compact the database, retaining only the last `keep` transactions.
/// Creates a new file with only reachable pages.
pub fn compact(self: *Database, keep: u64, output_path: []const u8) !void {
    // 1. Identify roots to keep (latest `keep` entries in root log)
    // 2. Mark-sweep: walk all reachable pages from those roots
    // 3. Copy reachable pages to new file, remapping page numbers
    // 4. Write new root log with remapped roots
}
```

This is analogous to `VACUUM` in SQLite or GC in a functional runtime.
It produces a new, smaller file. The old file can be deleted.

---

## Alternatives Considered

### A. Keep current design, add Tx to all index keys (pure append-only)

Store every assertion and retraction in all indexes with Tx+Op in keys.
No structural sharing, no persistent data structure. Current-state queries
filter at read time.

**Rejected because:** Every current-state query pays a filtering cost. For
entities with long histories, this is O(history length) per attribute lookup
instead of O(log N). The persistent tree approach gives O(log N) for all
access patterns.

### B. Keep current design with delete, add compaction later

Live with B+ tree delete and free pages. Add background compaction to
rebuild indexes periodically.

**Rejected because:** This requires the same background process we explicitly
don’t want (SQLite-inspired, no running processes). It also keeps the most
complex code paths (delete, merge, FreeDB) that this RFC eliminates.

### C. Hybrid: persistent trees + append-only indexes

Maintain both persistent current-state trees AND append-only full-history
indexes, giving optimal performance for all query patterns.

**Rejected for MVP because:** Double the storage and write amplification.
The persistent trees + TxLog approach (this RFC) covers all query patterns
with a single set of indexes plus a simple log. The full-history index can
be added later if history query performance matters.

---

## Open Questions

### Q1: mmap remapping on file growth

When the file grows (new pages appended), the mmap must be extended. Options:

- `mremap()` on Linux (atomic, fast)
- `munmap()` + `mmap()` on macOS (brief window where mapping is invalid)
- Over-map: mmap a large virtual region (e.g., 1TB), file grows into it

Recommend: Over-map with `MAP_NORESERVE`. Most OSes don’t allocate physical
pages until accessed. Access beyond file size produces SIGBUS, which we handle.

### Q2: Root log page overflow

When a root log page fills up, allocate a new one and link it. The file header’s
`root_log_tail` is updated to point to the new page. This means the file header
IS mutated (this one field). Alternative: keep root_log_tail in the last root
log page itself, making the header truly immutable after creation.

### Q3: Maximum history depth

At 76 bytes per root entry and one entry per transaction, 1 million transactions
consume ~73 MB of root log space. 100 million transactions: ~7.3 GB. This is
acceptable for the root log itself, but the unreclaimable tree pages could be
significant. Should we offer a `retainSince(tx_id)` that marks old roots as
droppable, enabling future compaction to reclaim pages only reachable from
dropped roots?

---

## References

- Okasaki, C. _Purely Functional Data Structures_ (1998) — foundational work
  on persistent data structures with structural sharing
- Bagwell, P. _Ideal Hash Trees_ (2001) — HAMT, basis for Clojure’s persistent maps
- Datahike — persistent hitchhiker trees on disk, closest prior art
  (https://github.com/replikativ/datahike)
- Tonsky, N. _Unofficial guide to Datomic internals_ (2014) — describes
  Datomic’s current/novelty/log index split
- LMDB — copy-on-write B+ tree design that inspired our original architecture
- ZatDB Implementation Design v0.2 — the spec this RFC modifies
